---
title: "Analiza eksploracyjna wybranych krajów za pomocą PCA i algorytmu grupowania."
author: "Aleksandra Jaworska, Mikołaj Broszczak"
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: yes
    code_folding: hide
  word_document:
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cluster)
library(ggrepel)
library(GGally)       
library(fpc)      
library(corrplot)
library(DataExplorer) 
library(plotly)       
library(FactoMineR)   
library(factoextra)
library(psych)
library(countrycode)
library(eurostat)
library(maps)
library(sf)
library(scales)
library(countrycode)
```


# Wprowadzenie
  
Celem projektu jest analiza eksploracyjna wybranych krajów za pomocą PCA i algorytmu grupowania. Źródłem zmiennych dla tej analizy była baza danych [Eurostatu](https://ec.europa.eu/eurostat/data/database). Nieliczne braki danych dla analizowanego roku (2019) zostały uzupełnione na podstawie średniej arytmetycznej wartości z pozostałych okresów dla badanego kraju.


## Opis wykorzystanych zmiennych 

```{r data, warning = FALSE, message = FALSE}
#Wczytanie zbioru i pozbycie się przejściowych kolumn służących do wyliczenia wskaźnika samobójstw 

suic <- read.csv2("./data/dane.csv")
suic <- suic[,-c(2,3,4)]
colnames(suic)[1] = "country"
colnames(suic)[7] = "general_diseases"

```

Zbiór analizowany w projekcie liczy 31 obserwacji oraz 7 zmiennych. 

* **Wskaźnik samobójstw w danym państwie** - zmienna powstała wskutek podzielenia [liczby samobójstw](https://ec.europa.eu/eurostat/databrowser/view/HLTH_CD_ARO__custom_5301739/default/table) danego kraju przez jego [populację](https://ec.europa.eu/eurostat/databrowser/view/TPS00001__custom_5302602/default/table) w danym roku; wskaźnik wyrażony w % populacji danego kraju 

* **[Wskaźnik bezrobocia danego państwa](https://ec.europa.eu/eurostat/databrowser/view/UNE_RT_A__custom_5301832/default/table)** - wskaźnik wyrażony w % populacji danego kraju; 

* **[Deprywacja materialna](https://ec.europa.eu/eurostat/databrowser/view/TESSI080/default/table)** - zmienna określająca brak możliwości zaspokojenia min. 3 z 9 potrzeb uznanych w warunkach europejskich za podstawowe, ze względu na problemy finansowe; zmienna wyrażona w % populacji danego kraju;

* **[Stopniodni ogrzewania (HDD)](https://ec.europa.eu/eurostat/databrowser/view/nrg_chdd_a/default/table?lang=en)** - wskaźnik stopniodni ogrzewania, mierzy dotkliwość chłodu w określonym przedziale czasu z uwzględnieniem temperatury zewnętrznej i średniej temperatury w pomieszczeniu; wyrażony w rocznej liczbie stopniodni grzania; 

* **[Wskaźnik wykonywanych nadgodzin](https://ec.europa.eu/eurostat/databrowser/view/LFSA_QOE_3A2$DV_608/default/table)** - zmienna przedstawiająca ilość osób wykonujących nadgodziny w swym głównym miejscu zatrudnienia; zmienna wyrażona w % wszystkich zatrudnionych osób w danym kraju; 

* **[Wskaźnik chorobowości](https://ec.europa.eu/eurostat/databrowser/view/HLTH_SILC_04__custom_5219456/default/table)** - zmienna przedstawiająca osoby chorujące na choroby przewlekłe; zmienna wyrażona w % populacji danego kraju; 

* **[Wskaźnik rowodów](https://ec.europa.eu/eurostat/databrowser/view/TPS00216/default/table)** - wskaźnik wyrażony w % populacji danego kraju; 


## Uzasadnienie doboru zmiennych 


* **Wskaźnik bezrobocia danego państwa** - istnieją badania, które sugerują, iż wysokie bezrobocie może wpływać na samopoczucie i stan psychiczny ludzi, a co za tym idzie zwiększać ryzyko samobójstw poprzez zwiększenie stresu i izolacji społecznej.

* **Deprywacja materialna** - badania wykazują, że deprywacja materialna, czyli brak środków do życia, takich jak pożywienie, mieszkanie i opieka medyczna, może wpływać na samopoczucie i zdrowie psychiczne ludzi, co może prowadzić do zwiększenia ryzyka samobójstw.

* **Stopniodni ogrzewania (HDD)** - istnieją badania, które sugerują, że zimowe warunki atmosferyczne mogą wpłynąć na poziom samobójstw. Wszystko dlatego, że niska temperatura i związana z nią izolacja mogą prowadzić do poczucia depresji, osamotnienia i braku nadziei

* **Wskaźnik wykonywanych nadgodzin** - badania sugerują, że wykonywanie nadgodzin może wpływać na zdrowie psychiczne, ponieważ prowadzi do zwiększonego stresu i zmęczenia, co z kolei może zwiększać ryzyko samobójstw.

* **Wskaźnik chorobowości** - choroby fizyczne i psychiczne mogą wpływać na zdolność do funkcjonowania i jakość życia ludzi, co może zwiększać ryzyko samobójstw.

* **Wskaźnik rowodów** - rozwody i rozpad związków mogą prowadzić do poczucia samotności, depresji i stresu, co z kolei może zwiększać ryzyko samobójstw.


# Podstawowe zależności między danymi


Z poniższej macierzy korelacji można dostrzec następujące zależności: 

* **Suicide_rate (wskaźnik samobójstw)** 
  + Ma ujemną korelację ze wskaźnikiem bezrobocia (-0,37), wskaźnikiem deprywacji materialnej (-0,33) oraz wskaźnikiem wykonywanych nadgodzin (-0,51) co sugeruje, że wyższe wartości tych zmiennych mogą zmniejszać       ryzyko popełnienia samobójstwa; 
  + Wskaźnik ten ma natomiast dodatnią korelację ze Stopniodniami ogrzewania (0,48), wskaźnikiem chorobliwości (0,41) oraz wskaźnikiem rozwodów (0,4) co może oznaczać, że w krajach z wyższymi wartościami HDD (a co      za tym idzie niższą temperaturą), większym odsetkiem osób cierpiących na choroby przewlekłe oraz większą ilością odnotowywanych rozpadów małżeństw, częściej dochodzi do samobójstw;

* **Unemployement_rate (wskaźnik bezrobocia)** 
  + Wykazuje dodatnią korelację ze wskaźnikiem deprywacji materialnej (0,46) i wskaźnikiem wykonywanych nadgodzin (0,43) co sugeruje, że w krajach gdzie obywatelom częściej doskwierają problemy finansowe i              niemożność zaspokojenia podstawowych potrzeb, występuje zarówno podwyższony poziom bezrobocia, ale również zwiększa się ilość osób wykonujących nadgodziny; może się to wiązać z chęcią owych osób do poprawy          sytuacji finansowej w gospodarstwie domowym, lub narzuconą ilością zadań przez pracodawce niemogącego zatrudnić nowych, wykwalifikowanych pracowników z powodu bezrobocia; 

* **HDD (Stopniodni ogrzewania)** 
  + Ma dodatnią korelację z wskaźnikiem samobójstw (0,48) oraz wskaźnikiem chorobliwości (0,42), co oznacza, że im mniejsza średnia temperatura (a zarazem większa wartość HDD), tym zwiększa się ilość samobójstw oraz     pogarsza się stan zdrowia;
  + Ma ujemną korelację ze wskaźnikiem deprywacji materialnej (-0,36), może to oznaczać, iż w krajach z niższą wartością HDD (a zarazem wyższymi temperaturami), niemożność zapewnienia godnego bytu jest na wyższym       poziomie; 

* **Divorce_rate (wskaźnik rozwodów)** 
  + Wskaźnik ma dodatnią korelację zarówno z wskaźnikiem samobójstw (0,41) jak i z wskaźnikiem chorobliwości (0,36), co sugeruje, że rozwody mogą wpływać negatywnie wpływać na zdrowie zarówno fizyczne jak i             psychiczne, zwiększając ryzyko zachorowania na choroby przewlekłe oraz popełnienia samobójstwa; 

```{r cor, warning = FALSE, message = FALSE}
# Obliczenie korelacji między zmiennymi za pomocą funkcji "cor" oraz wizualizacja korelacji za pomocą funkcji "corrplot".
cor(suic[,2:8])
corrplot(cor(suic[, 2:8]), order = "hclust", tl.cex = 0.7)
```

# Analiza PCA (Principal Component Analysis)

Analiza składowych głównych (PCA) służy do odkrycia prawidłowości między zmiennymi oraz redukcji liczby zmiennych opisujących zjawiska przy zachowaniu jak największej ilości oryginalnej zmienności. Analiza ta polega na wyznaczeniu składowych będących kombinacją liniową badanych zmiennych.


Po przeprowadzeniu *testu Bartletta* można zauważyć, iż warość **p-value = 0.0002152146** wynosi znacznie mniej niż założony poziom istotności 0,05. Należy zatem odrzucić hipotezę zerową mówiącą o tym, 
że macierz korelacji jest równa macierzy jednostkowej, a więc stwierdzić, że między zmiennymi występują zależności korelacyjne.

Wynik *testu KMO* równy **0,71** sugeruje, że zbiór danych jest odpowiedni do przeprowadzenia analizy czynnikowej. Wyniki MSA (Measure of Sampling Adequacy) dla poszczególnych zmiennych w tym przypadku mają wartości MSA powyżej 0,6, co również potwierdza, że są odpowiednie do analizy czynnikowej.

```{r test, warning = FALSE, message = FALSE}
# Wykonanie testu Bartletta, który służy do sprawdzenia hipotezy o tym, że macierz korelacji jest macierzą jednostkową (czyli zmienna niezależna nie jest skorelowana z żadną inną zmienną). 
cortest.bartlett(cor(suic[, 2:8]), n = nrow(suic)) 

# WYkonanie testu KMO (Kaiser-Meyer-Olkin), który służy do oceny przydatności analizy PCA dla danego zbioru danych.
KMO(suic[,2:8]) 

```


Po zbadaniu wartości własnych, ładunków oraz wykresu osypiska należy wybrać 2 składowe - wyjaśniają one 60,3% wariancji danych.

```{r principals, warning=FALSE, message = FALSE}
# Wykonanie PCA za pomocą funkcji "principal", gdzie "nfactors" określa liczbę składowych do utworzenia, a "rotate" określa sposób rotacji składowych. Wyniki PCA są zapisane w zmiennej "pr1".
pr1 <- principal(suic[,2:8], nfactors = 7, rotate = "none") 

# Wyświetlenie ładunków (loadings), które określają, jak mocno każda zmienna wpływa na daną składową oraz wartości własnych, które określają, jak wiele wariancji jest wyjaśniane przez każdą składową.
pr1$loadings 

# Wyświetlenie wykresu osypiska, który przedstawia wartości własne i pozwala określić, ile składowych należy wybrać (według kryterium Keisera, wybiera się te, które mają wartości własne większe niż 1).
plot(pr1$values, type="b") 
abline(h=1, col="red") 
plot(cumsum(pr1$values)/7, type="b") 
```


Po ponownym przeprowadzeniu redukcji wymiarów można wyodrębnić następujące składowe: 

* **Pierwsza składowa główna (PC1) - silniejszy wpływ czynników związanych z kondycją społeczną danego kraju.**
  + silna dodatnia korelacja ze wskaźnikiem samobójstw (0,8), Stopniodniami ogrzewania (0,71) oraz wskaźnikiem chorobliwości (0,59) - zmienne te mają największy wpływ na wyjaśnienie wariancji w PC1.
  
* **Druga składowa główna (PC2) - przeważający wpływ czynników ekonomicznych** 
  + silna dodatnia korelacja ze wskaźnikiem bezrobocia (0,62), wskaźnikiem rozwodów (0,63), wskaźnikiem deprywacji materialnej (0,38), wskaźnikiem wykonywanych nadgodzin (0,36) - zmienne te mają największy wpływ na wyjaśnienie wariancji w PC2


``` {r principal2, warning = FALSE, message = FALSE}
# Wykonanie PCA dla dwóch składowych i zapisanie wyników w zmiennej "pr2".
pr2 <- principal(suic[,2:8], nfactors=2, rotate="none") 

# Wyświetlenie ładunków składowych oraz wykresu biplot, który przedstawia zależności między zmiennymi oraz ich korelacje z dwoma pierwszymi składowymi.
pr2$loadings
biplot(pr2)
```

```{r PCAplot, warning=FALSE, message = FALSE}
suic.pca <- pr2$scores %>% 
  as.data.frame() %>% 
  mutate(country = suic$country)

options(ggrepel.max.overlaps = Inf)

suic.pca %>% 
  ggplot(aes(x = PC1, y = PC2)) +
  geom_text_repel(aes(label = country), size = 2.5) + 
  geom_point(shape = 21, fill = "#6D6BBD", size = 3) + theme_bw() + xlab("Kondycja społeczna kraju") + ylab("Czynniki ekonomiczne") 
```

# Grupowanie

Grupowanie ma na celu podzielenie zbioru obserwacji na podobne grupy, tak aby obserwacje w obrębie każdej grupy były podobne do siebie, a jednocześnie różniły się od obserwacji w innych grupach.Zostanie ono wykonane dwoma metodami: hierarchiczną oraz k-średnich.

## Grupowanie hierarchiczne

W grupowaniu hierarchicznym, obserwacje są stopniowo łączone w klastry, aż do osiągnięcia oczekiwanej ich liczby lub połączenia wszystkich obserwacji w jeden klaster.

```{r standaryzacja, warning = FALSE, message = FALSE}
# Przeprowadzenie standaryzacji zmiennych 

suic.scaled <- scale(suic[, 2:8]) %>% as.data.frame()
rownames(suic.scaled) <- suic$country

# sprawdzenie rezultatu
summary(suic.scaled) 
pairs(suic.scaled)
``` 

```{r macierz dystansu, warning = FALSE, message = FALSE, results='hide'}
# Stworzenie macierzy dystansu potrzebnej w celu przedstawienia odległości między obserwacjami w zbiorze danych.
ds <- dist(suic.scaled, method = "euclidean")
ds

# Gupowanie metodą warda ma na celu stworzenie hierarchii klastrów, które są jak najbardziej podobne do siebie wewnątrz, ale jak najbardziej różne między sobą na zewnątrz.
hc1s <- hclust(ds, method = "ward.D2")

```
### Dendogram

Dendrogram to graficzna reprezentacja hierarchii klastrów w grupowaniu hierarchicznym. Jest on przydatny w analizie grupowania hierarchicznego, ponieważ umożliwia wizualizację struktury klastrów i pomaga w wyborze optymalnego podziału.

```{r dendogram, warning = FALSE, message = FALSE}
# Wyswietlenie dendrogramu - plot na obiekcie hclust
plot(hc1s)
fviz_dend(hc1s, k = 4, rect = TRUE, color_labels_by_k = TRUE, cex = 0.5, palette = c("#FFEC8B","#87CEFF","#CD5555","#7CCD7C"))

# Wybór liczby grup
cutree(hc1s, k = 2) 
cutree(hc1s, k = 3)
cutree(hc1s, k = 4)
cutree(hc1s, k = 5) 
cutree(hc1s, k = 4) %>% table()

suic$cluster.w <- cutree(hc1s, k = 4) %>% as.factor()
```
Na podstawie powyższego dendogramu, można dojść do wniosku, że optymalną liczbą grup będzie 4, ponieważ odległość pomiędzy klastrami 4 i 5 jest największa. Również posługując się funkcję "cutree" dochodzimy do podobnego wniosku. 

## Grupowanie k-średnich

Polega ono na przypisaniu każdej obserwacji do jednej z k rozłącznych grup skupiających się wokół centroidów, czyli punktów  wyznaczających średnie wartości w każdej grupie.

```{r k-means, warning=FALSE, message = FALSE}
# Stworzenie obiektu klasy k-means

set.seed(10)
km1s <- kmeans(suic.scaled, centers = 4, nstart = 10)

km1s$cluster
km1s$withinss
km1s$tot.withinss

```
 
### Kryteria wyboru liczby grup

1. **WSS (Within-Cluster Sum of Squares)** - Kryterium to liczy sumę kwadratów odległości między każdym punktem, a środkiem ciężkości klastra i sumuje to dla wszystkich klastrów.

2. **Indeks Calińskiego-Harabasza** - Kryterium to mierzy stosunek różnicy między średnią odległością punktów wewnątrz klastra do średniej odległości punktów między klastrami. 

3. **Average Silhouette** - Dla każdego punktu obliczamy średnią odległość do punktów w tym samym klastrze i średnią odległość do punktów w najbliższym sąsiednim klastrze, a następnie obliczamy różnicę między nimi i dzielimy przez maksimum tych dwóch wartości.

```{r kryteria, warning=FALSE, message = FALSE}

# Stworzenie wykresu osypiska WSS
x <- rep(0, 10) 
# petla:
for(i in 1:10)
  x[i] <- kmeans(suic.scaled, centers = i, nstart = 10)$tot.withinss

# WYkres:
plot(x, type = "b")

# Kryterium ch (Calinskiego-Harabasza) 
kms.ch <- kmeansruns(suic.scaled, criterion = "ch", runs = 10)
plot(kms.ch$crit, type = "b")

# wniosek: 2 grupy

# Kryterium asw (Average Silhouette)
kms.asw <- kmeansruns(scale(suic.scaled), criterion = "asw", runs = 10)
plot(kms.asw$crit, type = "b")

# wniosek: 2 grupy


# Grupowanie według wszystkich kryteriów
fviz_nbclust(suic.scaled, clara, method = "wss") # 4 (ciężko określić)
fviz_nbclust(suic.scaled, clara, method = "silhouette") # 4
fviz_nbclust(suic.scaled, clara, method = "gap_stat") # 1 

```
```{r clusterboot, warning = FALSE, results='hide', message = FALSE}
# clusterboot

# hclust
clusterboot(suic.scaled, B = 500,
            clustermethod = hclustCBI, method = "ward.D2", k = 4)

# kmeans
clusterboot(suic.scaled, B = 500,
            clustermethod = kmeansCBI, krange = 4)

# clara
clusterboot(suic.scaled, B = 500,
            clustermethod = claraCBI, k = 4)
```

Wnioski: Na podstawie funkcji "clusterboot", która pozwala określić stabilność klasteryzacji, dochodzimy do wniosku, że najbardziej efektywne jest grupowanie hierarchiczne (hclust) dla czterech grup.

## Zestawienie wyników grupowań

```{r wyniki grupowań, warning = FALSE, message = FALSE}

 # Analiza wyników grupowania hierarchicznego:
suic %>% 
  pivot_longer(2:8) %>%
  group_by(cluster.w, name) %>% 
  summarise(mean.v = mean(value)) %>% 
  pivot_wider(names_from = name, values_from = mean.v)

plot_boxplot(suic[, 2:9], by = "cluster.w", ggtheme = theme_bw(), geom_boxplot_args = list("fill" = "#87CEFF"))

 # Analiza wyników grupowania metodą k-średnich:
suic$cluster.km <- km1s$cluster %>% as.factor()

suic %>% 
  pivot_longer(2:8) %>%
  group_by(cluster.km, name) %>% 
  summarise(mean.v = mean(value)) %>% 
  pivot_wider(names_from = name, values_from = mean.v)

suic$cluster.km <- as.factor(suic$cluster.km)


plot_boxplot(suic[, c(2:10)], by = "cluster.km", ggtheme = theme_bw(), geom_boxplot_args = list("fill" = "#7CCD7C"))

```



Na podstawie powyższych wyników formują nam się następujące grupy:

**Grupa 1** - Państwa niewyróżniające się. W ich przypadku żadna zmienna nie odstaje wyraźnie od pozostałych grup. W tej grupie znajduje się między innymi Polska.

**Grupa 2** - Grupa tych państw charakteryzuje się najniższym wskaźnikiem rozwódów oraz chorób przewlekłych, a przy tym wysoką deprywacją materialną. 

**Grupa 3** - Państwa skandynawskie oraz bałtyckie. W ich przypadku obserwujemy wysoką wartość HDD, co wynika z niskiej temperatury w tej części Europy. Oprócz tego najwyższy wskaźnik rozwodów, chorób przewlekłych oraz samobójstw. 

**Grupa 4** - Są to państwa najcieplejsze oraz posiadające największą deprywację materialną. Poza tym, wyróżnia je zdecydowanie największa liczba wykonywanych nadgodzin oraz najniższy wskaźnik samobójstw. 

Wyniki grupowań różniły się wyłącznie przyporządkowaniem jednego państwa - Cypru. W przypadku grupowania hierarchicznego znajdował on się w grupie 1, natomiast przy grupowaniu k-means w grupie 4.

```{r wizualizacja PCA, warning = FALSE, message = FALSE}

# Wizualizacja wynikow grupowania za pomoca PCA:

# grupowanie hierarchiczne:

sui1 <- suic %>% 
  select(-country,-cluster.km)
rownames(sui1) <- suic$country
pc1s <- PCA(sui1, quali.sup = 8, graph = FALSE)
fviz_pca_biplot(pc1s, habillage = 8, repel = TRUE, labelsize = 3)

# grupowanie k-means:

sui2 <- suic %>% 
  select(-country,-cluster.w)
rownames(sui2) <- suic$country
pc2s <- PCA(sui2, quali.sup = 8, graph = FALSE)
fviz_pca_biplot(pc2s, habillage = 8, repel = TRUE, labelsize = 3)

# dodatkowe wizualizacje dla grupowania hierarchicznego:

set.seed(10)
seta <- kmeans(suic.scaled, centers = 4, nstart = 10)

fviz_cluster(seta, data = suic.scaled, repel = TRUE, labelsize = 8)

fviz_cluster(seta, data = suic.scaled, repel = TRUE, labelsize = 8, 
             show.clust.cent = FALSE, ggtheme = theme_bw(), 
             ellipse.type = "norm", ellipse.level = 0.85)

```

Dodatkowo wyniki grupowania hierarchicznego postanowiliśmy zwizualizować za pomocą naniesienia kolorystyki odpoiwednich grup na wykres mapy Europy. 

```{r mapa, warning = FALSE, message = FALSE}

# Stworzenie tabeli  podstawowymi informacjami charakteryzującymi poszczególne państwa Europy. 
SHP <- get_eurostat_geospatial(resolution = 10, 
                               nuts_level = 0, 
                               year = 2021) 

# Usunięcie wierszy z informacjiami o krajach niezawartych w ogólnej analizie. 
SHP <- SHP[!(SHP$id %in% c("RS", "LI", "UK", "AL", "ME", "MK")),] 

# Przekształcenie oryginalnych państw krajów na ich angielskie odpowiedniki. 
SHP$NAME_LATN <- as.factor(SHP$NAME_LATN)
SHP$country<- countryname(SHP$NAME_LATN)
SHP[2, 13]<- "Belgium"
SHP[11,13]<- "Switzerland"
SHP[19, 13]<- "Greece"

# Sortowanie danych zgodne z porządkiem alfabetycznym.
SHP <- SHP[order(SHP$country), ]

# Wygenerowanie kolumny z angielskimi nazwami krajów w sui1.
sui1$country <- rownames(sui1)

# Przyporządkowanie grup do krajów w zbiorze SHP oraz zmiana nazwy kolumny na bardziej oczywistą. 
sui1$cluster.w <- as.factor(sui1$cluster.w)
SHP <- cbind(SHP, sui1$cluster.w)
colnames(SHP)[13] <- "group"

# Wizualizacja wyników 
SHP %>% ggplot(aes(fill = group)) + geom_sf() + scale_x_continuous(limits = c(-22, 35)) +
  scale_y_continuous(limits = c(35, 68)) + theme_bw() + 
  scale_fill_manual(values=c("#7CCD7C","#CD5555","#87CEFF","#FFEC8B")) 

# Na wykresie znajdują się tylko państwa uwzględnione w głównej analizie, stąd brak terytoium Wielkiej Brytanii, Rosji, Bośni i Hercegowniy, Czarnogóry, Albanii, Serbii. 
```

# Podsumowanie 

Przeprowadzona analiza PCA oraz grupowanie pozwoliły na zrozumienie struktury danych oraz zidentyfikowanie kluczowych zależności i grup wśród zmiennych. Wnioski płynące 
z przeprowadzenia obu analiz kształtują się następująco: 

*	Technika PCA pozwoliła wyodrębnić dwie składowe opisujące zmienne. Pierwszą składowa została opisana jako czynniki związane z kondycją społeczną kraju, natomiast druga składowa jako czynniki ekonomiczne.
* W procesie grupowania zostały zastosowane dwie główne metody: hierarchiczna oraz 
k-średnich. Na podstawie wyników funkcji „clusterboot” oraz własnych preferencji, grupowanie hierarchiczne zostało uznane za skuteczniejsze w tej analizie. Wyniki obu metod różniły się wyłącznie przypisaniem jednej obserwacji. 
*	Na podstawie dendrogramu oraz funkcji „cutree”, udało się wyznaczyć cztery główne grupy państw. Dzięki wizualizacji wyników na mapie Europy, można dojść do wniosku, że wartości poszczególnych zmiennych są związane z położeniem geograficznym danego kraju. 

Powyższe wyniki analizy PCA i grupowania są tylko początkiem procesu analizy danych 
i wymagają dalszej pracy w celu wyciągnięcia bardziej szczegółowych wniosków. Można je wykorzystać jako narzędzia eksploracyjne lub jako część większej analizy danych, np. do budowania modeli predykcyjnych.

